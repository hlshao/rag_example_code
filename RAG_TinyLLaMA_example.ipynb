{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a7d438",
      "metadata": {
        "id": "79a7d438"
      },
      "outputs": [],
      "source": [
        "# 安裝必要套件\n",
        "!pip install llama-cpp-python chromadb faiss-cpu PyMuPDF --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b78f69d1",
      "metadata": {
        "id": "b78f69d1"
      },
      "outputs": [],
      "source": [
        "# 下載 TinyLLaMA 1.1B Q4_K_M 量化模型 (~700MB)\n",
        "#!wget -q https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O tinyllama.gguf\n",
        "# 下載 Qwen1.5-0.5B-Chat Q4_K_M 量化模型（中文支援）\n",
        "#!wget -q https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q4_k_m.gguf -O qwen-0.5b-chat.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10dfd767",
      "metadata": {
        "id": "10dfd767"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# 載入模型\n",
        "llm = Llama(model_path=\"tinyllama.gguf\", n_ctx=2048, n_threads=4)\n",
        "#llm = Llama(model_path=\"qwen-0.5b-chat.gguf\", n_ctx=2048, n_threads=4)\n",
        "print(\"模型已載入完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42314bd1",
      "metadata": {
        "id": "42314bd1"
      },
      "outputs": [],
      "source": [
        "from chromadb import Client\n",
        "import chromadb.utils.embedding_functions as ef\n",
        "\n",
        "embedding_func = ef.DefaultEmbeddingFunction()\n",
        "chroma_client = Client()\n",
        "collection = chroma_client.create_collection(name=\"docs\", embedding_function=embedding_func)\n",
        "print(\"已建立 ChromaDB 檢索資料庫\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5206155d",
      "metadata": {
        "id": "5206155d"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        doc = fitz.open(filename)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    elif filename.lower().endswith(\".txt\"):\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "    else:\n",
        "        print(f\"不支援的檔案格式: {filename}\")\n",
        "        continue\n",
        "\n",
        "    # 分段加入向量資料庫\n",
        "    chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
        "    ids = [f\"{filename}_{i}\" for i in range(len(chunks))]\n",
        "    collection.add(documents=chunks, ids=ids)\n",
        "    print(f\"已處理並加入知識庫: {filename}\")\n",
        "\n",
        "print(\"文件已全部加入知識庫\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "919b3282",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "919b3282",
        "outputId": "299f44d9-48c3-422d-de1b-bb3f0e976422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 22 prefix-match hit, remaining 1474 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   10062.18 ms\n",
            "llama_perf_context_print: prompt eval time =   51260.33 ms /  1474 tokens (   34.78 ms per token,    28.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =   21289.05 ms /   255 runs   (   83.49 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:       total time =   72706.28 ms /  1729 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "一、 備註：\n",
            "本表僅依訂務單位所屬委員會，均無任何沿遷或項目內容僅限內部參與。\n",
            "本表是由於該委員會主要工作及經費管理，具有大部分內部交付，其\n",
            "細節規範，本表項目內容僅依訂務單位所屬委員會項目略稱，其參與\n",
            "其他委員會條約，僅在必要時，納約沿遷。\n",
            "二、 考核方式：\n",
            "本表中各列該費\n"
          ]
        }
      ],
      "source": [
        "def rag_query(query):\n",
        "    results = collection.query(query_texts=[query], n_results=3)\n",
        "    context = \" \".join(results[\"documents\"][0])\n",
        "    prompt = f\"\"\"使用以下資料回答問題：\n",
        "{context}\n",
        "\n",
        "問題：{query}\n",
        "回答：\n",
        "\"\"\"\n",
        "    output = llm(prompt, max_tokens=256, stop=[\"</s>\"])\n",
        "    return output[\"choices\"][0][\"text\"]\n",
        "\n",
        "# 測試\n",
        "print(rag_query(\"國家科學及技術委員會中的經費規定\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}